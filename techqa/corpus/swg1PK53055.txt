Title: IBM PK53055: QUEUE MANAGERS ARE IN A FAILED-PERSISTENT STATE IN THE CSQ_ADMIN STRUCTURE - United States

Text:
z/os  A FIX IS AVAILABLE
Obtain the fix for this APAR.


SUBSCRIBE
You can track all active APARs for this component.



APAR STATUS
 * CLOSED AS PROGRAM ERROR.
    
   
   

ERROR DESCRIPTION
 *  The customer explained their problem as a spin loop on their
   system resulting in several queue managers that were in a
   failed-persistent state in the CSQ_Admin structure. In order to
   get out of the situation they had to bring down all of our queue
   managers that recieved an IXL041E error.
   IXL041E CONNECTOR NAME: CSQEMJP1PQ040C, JOBNAME: PQ04MSTR, AS
   HAS NOT RESPONDED TO THE DISCONNECTED/FAILED CONNECTION EVENT
   SUBJECT CONNECTION: CSQEMJP1PQ0206.
   DISCONNECT/FAILURE PROCESSING FOR STRUCTURE MJP1SYSDATA
   CANNOT CONTINUE.
   MONITORING FOR RESPONSE STARTED: 05/29/2007 13:09:58.
   DIAG: 0000 0000 00000000
   .
   The Change Team has determine the sequence of events which led
   to the hang, and we have been able to recreate the failure on
   our test systems. The sequence of events is as follows:
   1) PQ02 and PQ31 fail, and peer recovery is started on all queue
   managers currently connected to each of the structures.
   2) While the USYNC which drives peer recovery is still active,
   PQ03 attempts to connect to the SYSDATA structure to satisfy an
   open request on SYSTEM.QSG.CHANNEL.SYNCQ.
   3) PQ03 is notified on the response to the IXLCONN that a USYNC
   is outstanding (CONAUSYNCEVENTSET is on). As a result, PQ03
   schedules peer recovery processing, but allows open processing
   to continue.
   4) As part of the open processing, we attempt to lock the list
   header for the queue. The IXLLSTC request causes XES to mark the
   lock as requested by PQ03, but it won't actually allocate it to
   PQ03 until the peer recovery is completed and the outstanding
   events have been acknowledged.
   5) During peer-level recovery, PQ03 and all the other queue
   managers attempt to get the lock on the same list header (as its
   marked as requiring work for the failed connections). The state
   of the lock after step 4 causes XES to return
   IXLRSNCODELOCKHELDBYSYS.
   6) Peer level recovery processing treats IXLRSNCODELOCKHELDBYSYS
   as a temporary state, and makes an unconditional request for the
   lock.
   7) All the queue managers performing recovery now have their
   worker thread for the SYSDATA structure hung waiting for the
   list header lock. However XES won't make the lock available
   until the events are acknowledged, so we have a deadlock.
   .
   It looks like the peer-level recovery code needs to handle the
   IXLRSNCODELOCKHELDBYSYS reason code differently, so that
   peer-level recovery can complete.
   
   
    
   
   

LOCAL FIX

PROBLEM SUMMARY
 *  ****************************************************************
   * USERS AFFECTED: All users of Websphere MQ for z/OS Version 6 *
   ****************************************************************
   * PROBLEM DESCRIPTION: In an MQ Shared Queue environment where *
   *                      the QSG consists of more than two       *
   *                      QMGRs, after a QMGR fails, Peer         *
   *                      Level Recovery processing on one or     *
   *                      more of the surviving QMGRs may hang    *
   *                      causing messages, IXL041E due to        *
   *                      EEPLDISCFAILCONN events being queued    *
   *                      and not processed.                      *
   ****************************************************************
   * RECOMMENDATION:                                              *
   ****************************************************************
   A hang can occur in QSG peer level recovery as a result of
   waiting indefinitely for a lock which won't be released until
   PLR is complete.
   
   In Phase 2 PLR, module CSQERWI2, we try to get an exclusive lock
   on a queue's list header (proc Lock_List_Header).  If the
   request fails with ixl_rsn_code of IxlRsnCodeLockHeldBySys, then
   we request the lock unconditionally (in proc Lock_List_Uncond) -
   which constitutes a serialized request for the lock in XES.
   
   Analysis by the XCF team identified a situation in which a lock
   could be in the process of being given to a queue manager, but
   wouldn't complete that processing until everyone had
   acknowledged an earlier EEPLDISCFAILCONNECTION event.  This
   results in the IxlRsnCodeLockHeldBySys reason code (rather than
   IxlRsnCodeLockCond, which is returned if the other queue manager
   had successfully got the lock).
   
   XCF won't complete the lock processing until all participants
   have acknowledged the failed event, and we won't acknowledge the
   event until we've finished peer level recovery.
   
   This causes the structure task on each queue manager involved in
   PLR to hang on the unconditional lock request which will never
   complete.
   
   To resolve this deadlock, the customer needed to start
   cancelling queue managers until they found the one that had the
   incomplete lock processing.
   
   We should not wait indefinitely on the lock during PLR, but
   respond to the discfailconn to indicate that the failed QMGR
   should be left failed persistent in order to recover itself on
   restart, as we cannot complete the recovery without the list
   header.
   
   
    
   
   

PROBLEM CONCLUSION
 *  The code has been changed so that instead of requesting the list
   header lock unconditionally during phase 2, we use conditional
   requests, each a second apart.  If the lock can't be obtained
   after 5 further attempts we fail PLR to allow the expected
   response to the EEPLDISCFAILCONN to complete.  In such an event
   we also issue message CSQE034E to indicate failure to complete
   phase 2.
   000Y
   CSQERWI2
   
   
    
   
   

TEMPORARY FIX
 *  *********
   * HIPER *
   *********
   
   
    
   
   

COMMENTS
 *  Å¾**** PE09/03/10 FIX IN ERROR. SEE APAR PK74644 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK74644]  FOR DESCRIPTION
   
   
    
   
   

APAR INFORMATION
 * APAR NUMBER
   PK53055
   
   
 * REPORTED COMPONENT NAME
   WMQ Z/OS V6
   
   
 * REPORTED COMPONENT ID
   5655L8200
   
   
 * REPORTED RELEASE
   000
   
   
 * STATUS
   CLOSED PER
   
   
 * PE
   NoPE
   
   
 * HIPER
   YesHIPER
   
   
 * SPECIAL ATTENTION
   NoSpecatt
   
   
 * SUBMITTED DATE
   2007-09-18
   
   
 * CLOSED DATE
   2007-10-24
   
   
 * LAST MODIFIED DATE
   2009-03-11
   
   

 * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:
    PK48502 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK48502]
   
   
 * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:
    UK30584
   
   

MODULES/MACROS
 *     CSQERWI2
   
   
    
   
   

FIX INFORMATION
 * FIXED COMPONENT NAME
   WMQ Z/OS V6
   
   
 * FIXED COMPONENT ID
   5655L8200
   
   

APPLICABLE COMPONENT LEVELS
 * R000 PSY UK30584 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK30584]
   UP07/11/07 P F711
   
   

FIX IS AVAILABLE
 * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.