Title: IBM Frequent crash dumps generated by Sterling B2B Integrator (SBI) - United States

Text:
 TECHNOTE (TROUBLESHOOTING)

PROBLEM(ABSTRACT)
 At least once a day the application -- SBI -- generates a set of crash dumps in the <install>/noapp/bin directory, similar to that of an OutOfMemory (OOM) error. 



SYMPTOM
 

- noapp.log shows that crash dumps have been generated 

- <install>/noapp/bin directory contains crash dump files, which are: 

javacore.<date+time>.txt
core.<date+time>.dmp
heapdump.<date+time>.phd
Snap.<date+time>.trc


CAUSE
Older version of JGroups connects to and sends its information to a newer version of JGroups (JGroups is used by SBI for cluster communication). 


ENVIRONMENT
Redhat Enterprise Linux (RHEL)



DIAGNOSING THE PROBLEM
- When looking at the javacore, "Current thread" shows the following stack: 


1XMCURTHDINFO Current thread
NULL ----------------------
3XMTHREADINFO "ConnectionTable.Connection.Receiver [xx.xx.xx.33:xxxxx - xx.xx.xx.85:xxxxx]" J9VMThread:0x000000007BC19700, j9thread_t:0x00007F7309EE4E90, java/lang/Thread:0x000000001EEC9E50, state:R, prio=5
3XMJAVALTHREAD (java/lang/Thread getId:0x87E, isDaemon:false)
3XMTHREADINFO1 (native thread ID:0x4E3A, native priority:0x5, native policy:UNKNOWN)
3XMTHREADINFO2 (native stack address range from:0x00007F72F0930000, to:0x00007F72F0971000, size:0x41000)
3XMCPUTIME CPU usage total: 5.105312185 secs
3XMHEAPALLOC Heap bytes allocated since last GC cycle=0 (0x0)
3XMTHREADINFO3 Java callstack:
4XESTACKTRACE at org/jgroups/blocks/BasicConnectionTable$Connection.run(BasicConnectionTable.java:585)
4XESTACKTRACE at java/lang/Thread.run(Thread.java:804)
3XMTHREADINFO3 Native callstack: 

 

'GC History' section shows the following memory allocation request: 

------------------------
3STHSTTYPE 04:14:55:728094000 GMT j9mm.101 - J9AllocateIndexableObject() returning NULL! 1650814064 bytes requested for object of class 0000000079D7F200 from memory space 'Generational' id=00007F63B4038680 

 

- Heapdumps are very small and don't show any signs of memory exhaustion, eliminating the suspicion of an OutOfMemory (OOM) error 

 

- Jetty log contains the following message: 


[2015-09-30 16:52:37.672] ALL 000000000000 GLOBAL_SCOPE 10070619 [OOB-1,Sterling_NodeInfo_group,testserver-51074] DEBUG org.jgroups.protocols.pbcast.STABLE - testserver-51074: received digest from testserver-3431 (digest=testserver-3431: [2008 (2013)], testserver-51074: [1982 (1985)]) which does not match my own digest (testserver-51074: [2000 (2000)]): ignoring digest and re-initializing own digest

* NOTE: This may not always be an indication of this problem since the nodes of a cluster may, in fact, have differing digests at some point, but it is a good clue to look for if the above symptoms also match. 


- The particular system in this test case was only clustered in the sense that it had an adapter container (AC) configured on it. There was no node2. Only node1 and node1AC1. Typically, the default UDP multicast setting may catch interference from other clusters in the subnet, but after changing the UDP multicast address and even changing JGroups communication to TCP, the issue still persisted. 



RESOLVING THE PROBLEM
The system that was generating the crash dumps was a new SBI 5.2.5 install, which contained new JGroups code. It turned out that there was another identical system on the same subnet with the older version of SBI, and subsequently older JGroups, which was trying to communicate with this new 5.2.5 instance. This instance was set up on the same port as the new one. And because the two JGroups versions did not know how to properly "understand" each other, the newer version was causing the JVM to write out crash dumps. 

 

The resolution, then, can be one of the following: 

1. Disable the old version of SBI 

2. Create network isolation between the different systems, such as with a physical or software firewall 

3. Move one of the systems to a different subnet 

4. Increase the heap size by at least 1.6 GB (the size of the request) to allow the JVM to absorb the request and not have to crash. 

5. Isolate JGroups based on Group Name 

####customer_overrides.properties###
jgroups_cluster.group_name=Sterling_NodeInfo_group_PROD

or

####customer_overrides.properties###
jgroups_cluster.group_name=Sterling_NodeInfo_group_DEV

https://docs.jboss.org/jbossas/docs/Clustering_Guide/4/html/ch07s07s08.html [https://docs.jboss.org/jbossas/docs/Clustering_Guide/4/html/ch07s07s08.html] 

 

*NOTE: Changing JGroups initial_hosts parameter can't work in this case because there is only 1 host on each side in distribution_property_string (Adapter Containers [ACs] are not included in this string), and the two systems find each other through GOSSIP.