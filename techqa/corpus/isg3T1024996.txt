Title: IBM How does LSF allocate GPU's? - United States

Text:
 TECHNOTE (TROUBLESHOOTING)

PROBLEM(ABSTRACT)
 Since 9.1.3, LSF starts supporting parallel jobs that request GPU's, while LSF does not support the affinity of the GPU's though CPU affnitiy is supported. This article explains how LSF allocate GPU's for parallel jobs. 

RESOLVING THE PROBLEM
LSF always try to allocate CPU's and GPU's from the same PCI bus when possible. 

By default, LSF allocate GPU resources PER_HOST according to the resource requirement string. Refer to "man lsb.resources" for the other resource reservation method than PER_HOST. 

For jobs requesting Shared mode GPU's (submitted with -R "rusage[ngpus_shared=N]"), all GPU's in Shared mode on each exec host will be visible to the jobs, whatever the value of N is, since Shared mode GPU's are supposed to be shared by all jobs. However LSF does spread the workload to all Shared GPU's on each host in a round robin way. 

For jobs requesting Exclusive Process mode (excl_p) GPU's (submitted with -R "rusage[ngpus_excl_p=N]"), exact N excl_p GPU's on each exec host will be allocated to the jobs. 



RELATED INFORMATION
 Define GPU resources [https://www.ibm.com/support/knowledgecenter/SSETD4_9.1.3/lsf_admin/define_gpu_resources.html]