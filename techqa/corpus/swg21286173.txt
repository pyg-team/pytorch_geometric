Title: IBM ICM query: Handling large results sets on large archives - United States

Text:
large results; large result sets; big results; huge result TECHNOTE (FAQ)

QUESTION
 My archive is very large and the typical size of search or query results is very large. My archives will grow to the hundreds of millions (and can be as much as 500 million a year). When a user searches, the user will search collections of item types that can contain millions of documents. Searches will contain a date range in addition to other criteria, so result sizes are not likely to be 1 million. However, a result set of 200,000 is certainly possible and 50,000 or more is the norm. My application's job is to display a page of results in a reasonable amount of time (usually 1-2 seconds). What can you recommend about the query method that I should use? Today I just have a cursor and get the results one at a time. Is this optimal? 

ANSWER
It sounds like you have large result sets and are primarily interested in showing the first page of results as fast as possible. There are a number of ways you can do this, each of which has advantages.
Use DKDatastoreICM::execute()
The simplest way is to use DKDatastoreICM::execute(), which returns a dkResultSetCursor. This method has the effect of only instantiating DDOs for the first page of results. The smart cursor is optimized, so you are not forcing a retrieve call for each fetch(). Execute is an open cursor to the database, and when you make the first fetch() request, it will load a whole "block" of results, creating one block of empty DDOs with completed PIDs. Depending on your retrieve options, it can call multi-item retrieve on that block of DDOs. So when you call fetchNext() the second time, the function searches for items that are left in a block that is already retrieved and it just returns the next DDO from memory. If you call fetchNext() and there are none left in any prefetched block, then it loads another block, creating another block of blank DDOs with completed PIDs, and then it sends them to multi-item retrieve. You can configure the prefetch block size through options, but the default tends to work effectively. Be careful of setting the block size too large because if you cross past the maximum batch size that multi-item retrieve uses internally, it can be slightly less efficient than it could be. The retrieve batch size is not documented or guaranteed and can change over time, but as a general rule you should choose block sizes of 300 or smaller or use the default. So you should not worry about fetching one by one from the cursor as long as you are not calling single-item retrieve yourself on each individually. The benefits here are that if you have 50,000 results, you only instantiate and retrieve 50 - 300 DDOs. The API does not even hold PID information in memory for the other 49,950. One extra function you can do is to always fetch the first DDO on the next page so that you know if a next page exists. Therefore, if one exists, then you can display a "more" button or icon. A drawback to using this function is that you have a connection open the whole time.

Use Evaluate()
Evaluate() instantiates empty DDOs with completed PIDs for all results before returning. You have to wait for the query to read 50,000 PIDs from the library server results, create 50,000 DDOs in memory, and set the PIDs. Depending on your retrieve options it can call multi-item retrieve on the 50,000 DDOs. This is not likely to be a good choice for many applications.

Retrieving 50,000, 100,000, or 500,000 items even to list query results seems unusual. For typical applications, a user is not very likely to actually page through thousands and thousands of pages. Think of searching the Web through a popular search engine. If you get 100,000 hits, how likely are you to page all the way to the last one? You should consider setting a maximum results option to something reasonable. Perhaps you can ask for one more than you want so that you can know if there are more. Even for execute(), you can possibly help the speed of the transaction by truncating the number of results for any number beyond reasonable counts that would ever be used.

Another more complex solution is to use evaluate() and do your own blocks of results. Set the maximum results to a number such as 100, and SORTBY ITEMID. So you get the first 100. If the user wants the next hundred, re-run the same query, but this time put "@ITEMID > insertLastItemIdOfLastBlock" in the predicate so that you get the next 100 in sorted item ID order. This is also a good technique for failure recovery even for use of the execute() method. Suppose that you paged through 25,459 results of 50,000 and the connection goes down. How can you restart your query and continue where you stopped? If you had sorted by ITEMID and knew the item ID where you stopped, you could run a query for the remaining results after that item ID. The TExportManagerICM sample tool does this so it can automatically reconnect and retry or can be restarted later automatically from tracking files. Note that there are other online support documents detailing this algorithm.

Use callback or cancellable callback
Another solution you can consider is to use callback or cancellable callback (with DKDatastoreICM::executeWithCallback()). Using this method requests the API to fork a separate thread which calls methods on your object instance of a class you implement following a defined interface whenever some results are available. Your object is called and passed some more results as they come in. You can display results as soon as they come in and display more as more results are passed to your object over time. However, you will still get the whole 50,000 or 100,000, and the size of that result set can be a problem if you do not set a limit.




Cross reference information Segment Product Component Platform Version Edition Enterprise Content Management Content Manager DB2 Information Integrator for Content AIX, Red Hat Linux, Solaris, Windows 8.2, 8.3 All Editions