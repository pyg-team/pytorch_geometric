Title: IBM LSF Reports CPU Time For A Job Much Larger Than The Time Command - United States

Text:
cputime realtime usertime TECHNOTE (FAQ)

QUESTION
 Why is the CPU time usage reported by LSF much larger than the CPU time usage reported by the "time" command when running the same application? 

CAUSE
The time command only monitors CPU utilization until the root processes exits.

ANSWER
The time command is basic and will only collect the CPU time utilization of the root process and child processes until the root process exits. 

On the other hand, LSF will monitor and collect CPU time utilization of the root job process and all child processes until all processes exit. LSF will not stop monitoring CPU time usage if the root process exits but there are still child processes running.

For example, the following script (~/timetest.sh) runs the "sleep 60" command as a child process and then immediately exits.


 * #!/bin/bash
 * 
 * 
 * sleep 60&
 * 

The time command does not take into account the child process once the root script process exits. The real time is reported as a fraction of a second (0.013): 
 * bash-4.1$ time ~/timetest.sh
 * 
 * 
 * real 0m0.013s
 * 
 * user 0m0.000s
 * 
 * sys 0m0.007s


However when running the script using LSF, LSF is able to keep track of the child sleep process while it is alive even though the root process has exited. As a result the job is reported as being running for 64 seconds: 
 * bash-4.1$ bsub ~/timetest.sh
 * 
 * Job <3165> is submitted to default queue <normal>.
 * 
 * bash-4.1$ bhist 3165
 * 
 * Summary of time in seconds spent in various states:
 * 
 * JOBID USER JOB_NAME PEND PSUSP RUN USUSP SSUSP UNKWN TOTAL
 * 
 * 3165 test *test.sh 10 0 64 0 0 0 74


The same idea applies to CPU time utilization. LSF is able to record complete process statistics even though the root process has exited. 


Cross reference information Segment Product Component Platform Version Edition IBM Spectrum Computing IBM Spectrum LSF IBM Spectrum Computing IBM Spectrum LSF