Title: IBM Primary namenode failing with "Too many open files" error - United States

Text:
dropped failed TECHNOTE (TROUBLESHOOTING)

PROBLEM(ABSTRACT)
 Primary namenode dropping while jobs are running after awhile 

SYMPTOM
hadoop-hdfs-namenode.log entry

(Slf4jLog.java:warn(89)) - EXCEPTION 
java.io.IOException: Too many open files
at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:241)
at org.mortbay.jetty.nio.SelectChannelConnector$1.acceptChannel(SelectChannelConnector.java:75)
at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:686)
at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)



CAUSE
The jobs are generating too many files, exceeding the nofiles parameter in 

ENVIRONMENT
Linux

DIAGNOSING THE PROBLEM
Monitor the open files using linux command > lsof to see the number of files that are being created.




Choose the nofile setting based on these findings


RESOLVING THE PROBLEM
Connect to the node that is running ambari-server 

1) Edit /var/lib/ambari-server/resources/stacks/BigInsights/4.0/services/HDFS/package/templates/hdfs.conf.j2

2) Specify values based on your findings when you monitored the number of files (lsof) for example: 

{{hdfs_user}} - nofile 130000
{{hdfs_user}} - nproc 65536

3) Stop Ambari agent and Ambari server on the node where you have Namenode

ambari-agent stop
ambari-server stop

4) Delete ambari agent cache

rm -rf /var/lib/ambari-agent/cache/*

5) Start Ambari server / Ambari Agent

ambari-server start
ambari-agent start

4) Under ambari agent cache check for hdfs.conf.j2 file

# cat var/lib/ambari-agent/cache/stacks/BigInsights/4.0/services/HDFS/package/templates/hdfs.conf.j2 
... (When Ambari restarts it should update this file to match the one modified in step 1)

{{hdfs_user}} - nofile 130000
{{hdfs_user}} - nproc 65536

5) Restart HDFS from Ambari UI

6) To verify the settings have been made, run the command for number of files for Namenode process

# cat /proc/`ps aux | grep -v grep | grep org.apache.hadoop.hdfs.server.namenode.NameNode | awk '{print $2}'`/limits 


Limit Soft Limit Hard Limit Units 
Max cpu time unlimited unlimited seconds 
Max file size unlimited unlimited bytes 
Max data size unlimited unlimited bytes 
Max stack size 10485760 unlimited bytes 
Max core file size unlimited unlimited bytes 
Max resident set unlimited unlimited bytes 
Max processes 65536 65536 processes
Max open files 130000 130000 files 
Max locked memory 65536 65536 bytes 
Max address space unlimited unlimited bytes 
Max file locks unlimited unlimited locks 
Max pending signals 515028 515028 signals 
Max msgqueue size 819200 819200 bytes 
Max nice priority 0 0 
Max realtime priority 0 0 
Max realtime timeout unlimited unlimited us