Title: IBM Collecting data for Sterling Commerce Performance - United States

Text:
 TECHNOTE (TROUBLESHOOTING)

PROBLEM(ABSTRACT)
 Collecting data for problems with the Sterling Commerce performance. Collecting this MustGather information before calling IBM support will help you understand the problem and save time analyzing the data. 

RESOLVING THE PROBLEM


COLLECTING DATA
Identified problems  1. Application server/agent server/integration is frozen or almost non-responsive  * 
     * Threads are stuck at database due to long running queries or due to blocking locks. 
     *  Long running processes taking significant time and/or too many threads running in parallel. 
     *  Stale entries in the database slowing down the processes with trying to ping every URL in yfs_heartbeat during cache refresh/broadcast. 
     *  Any network related issues 
     *  Frequent Full GCs of very long interval 
    
     * 
     * 3 to 4 sets of Thread dumps collected at intervals of 20 to 30sec should give clarity on why and where in the process the threads are stuck i.e., At database, performing cache refresh, or any other reason. Note: Most JVMs redirects thread dumps to system out logs. It is necessary to redirect system out of JVM process to some log file to obtain thread dumps. for example, java > log_file 2>1& IBM java redirects thread dump to its own file. Look for an output file in the installation root directory with a name like javacore.date.time.id.txt 
     *  If blocking lock issue at DB, 'Blocking Lock SQL' output results need to be analyzed understand the processes, tables and the queries involved in blocking. 
     *  If overall DB slowness but not due to blocking locks, AWR report can be analyzed to identify the specific SQLs causing the issue. 
     *  If cache refresh is the problem, number RMI threads can be checked to see if the number is very high. yfs_heartbeat table records can be verified to check if there are too many stale entries. Stale entries can be updated temporarily. yfs_heartbeat can be truncated when all app/agent JVMs are down. Health monitor should run to avoid the situation in future. 
     *  If memory/GC is the problem, increasing the heapsize or tuning the GC parameters would help 
    
    
 2. Application/agent/integration server crash, shutdown  * 
     * Out-Of-Memory - Java heap space - causing the server crash 
     * OOM - StackOverFlow - causing the server crash 
     * Abnormal kill signal issues by external process/script/OS level  * 
        *  If OOM Java heap space, Memory parameters (Xmx, Xms), GC logs, thread stack (logs) and heapdumps need to be collected. 
        *  If OOM Stack-Overflow, Memory parameters (Xmx, Xms, Xss), Error logs and thread dumps need to be collected => Custom code issue or tuning Xss could help in most occasions. For waves with large number of shipments (nWMS), often seen that Tuning Xss helps (documented in perf guide). 
        *  Server level logs would help understanding if there are any external issues causing the crash. 
       
       
    
    
 3. Agent process is very slow, backlog of messages  * 
     *  Agent processing messages at very slow rate due to overall high time consumption => DB queries slow or long running process with additional services, external calls. 
     * Frequent Full GCs due to memory issues 
    
     * 
     * SQLDEBUG logs of the agent. 
     * AWR report 
     *  GC logs. 
    
    
 4. Database: Too many blocking locks  * 
     *  SELECT for UPDATE queries fired by agent/application server are taking too much time waiting for acquiring lock. 
    
     * 
     * Output of blocking lock SQLs 
     * AWR report 
     * Once above information is obtained, kill root blocker from DB side. This process will generate error stack in IBM Sterling Application logs. 
    
    
 5. DB high CPU, slow response  * 
     *  Slow response from database for the query fired by the product 
    
     * 
     *  AWR Report 
    
    
 6. Deadlocks from application and agent servers  * 
     * Deadlock can occur between two or more processes 
    
     * 
     *  Yantra Deadlock logs, SQLDEBUG logs for the processes identified 
     *  Oracle deadlock logs (if yantra deadlock logs not available) and AWR report 
    
    
 7. Application/Queue specific errors in agent logs  * 
     * Error details 
    
    If possible, set yfs.app.identifyconnection=Y in customer_overrides.properties while collecting diagnostic data from database (blocking lock SQLs or AWR report or yantra deadlock trace). This is diagnostic property and has overhead. Be cautious when using this property in production environment. 
    
    
    Blocking Lock SQLs Property to enable: 
    yfs.app.identifyconnection=Y in yfs.properties file for 7x
    yfs.yfs.app.identifyconnection=Y in customer_overrides.properties file for 8x and above
    Following SQL will work only for Oracle Database
    
    QUERY-1:
    SELECT sysdate,lh.inst_id l_ins, lh.SID lock_sid, lx.client_info||lx.module Locker,
    lx.status Locker_Status, lx.SQL_HASH_VALUE, lx.PREV_HASH_VALUE, ls.client_info||ls.module 
    Waiter,
    ls.status Waiter_Status, ls.last_call_et, do.object_name, lw.inst_id w_ins,
    lw.SID wait_sid,
    DECODE (lh.TYPE,
    'MR', 'Media_recovery',
    'RT', 'Redo_thread',
    'UN', 'User_name',
    'TX', 'Transaction',
    'TM', 'Dml',
    'UL', 'PLSQL User_lock',
    'DX', 'Distrted_Transaxion',
    'CF', 'Control_file',
    'IS', 'Instance_state',
    'FS', 'File_set',
    'IR', 'Instance_recovery',
    'ST', 'Diskspace Transaction',
    'IV', 'Libcache_invalidation',
    'LS', 'LogStaartORswitch',
    'RW', 'Row_wait',
    'SQ', 'Sequence_no',
    'TE', 'Extend_table',
    'TT', 'Temp_table',
    'Nothing-'
    ) waiter_lock_type,
    DECODE (lw.request,
    0, 'None',
    1, 'NoLock',
    2, 'Row-Share',
    3, 'Row-Exclusive',
    4, 'Share-Table',
    5, 'Share-Row-Exclusive',
    6, 'Exclusive',
    'Nothing-'
    ) wait_mode_req
    FROM gv$lock lw, gv$lock lh, gv$session ls, gv$session lx, gv$locked_object lo, dba_objects do 
    WHERE lh.id1 = lw.id1
    AND lh.id2 = lw.id2
    AND lh.request = 0
    AND lw.lmode = 0
    AND lw.inst_id=ls.inst_id
    AND lx.inst_id=lh.inst_id
    AND lw.sid = ls.sid
    AND lx.sid = lh.sid
    AND lx.inst_id = lo.inst_id
    AND lx.sid = lo.session_id
    AND lo.object_id = do.object_id
    AND (lh.id1, lh.id2) IN (SELECT id1, id2
    FROM gv$lock
    WHERE request = 0
    INTERSECT
    SELECT id1, id2
    FROM gv$lock
    WHERE lmode = 0)
    order by lh.inst_id, lh.sid;
    
    QUERY-2:
    select /*+ ordered */ sysdate, s.inst_id, sid, client_info, module, s.status, machine,
    program, logon_time, last_call_et, lo.object_id, object_name
    from gv$session s, gv$locked_object lo, dba_objects o
    where type = 'USER' AND
    last_call_et > 15 AND
    s.sid = lo.SESSION_ID AND
    s.inst_id = lo.INST_ID AND
    lo.object_id = o.object_id
    order by s.inst_id, s.sid;
    Deadlock Trace 
    
     * 
     * yfs.app.identifyconnection=Y in yfs.properties file for 7x 
     * yfs.yfs.app.identifyconnection=Y in customer_overrides.properties file for 8x and above 
    
     Enable Deadlock Parameter (applicable only for Oracle while troubleshooting deadlock issues) Add JVM parameter -DDLOCK_LOG_DIR to All IBM Sterling JVMs.
    -DDLOCK_LOG_DIR=/directory/where/deadlock/logs/should/go/.
    In addition, if Schema Owner and DB login user are different then also add -DOWNER= 
    For example, if Schema Owner is YANTRA then add following parameter 
    -DOWNER=YANTRA 
    If JVM encounters deadlock the above parameter will generate deadlock report in log directory configured using -DDLOCK_LOG_DIR parameter. This logs will have details of all the processes involved in deadlock situation. 
    
    
    
    
    
    3. COLLECT DATA
     * Gather all files.
     *  Collect all files, traces and compress to one file using the PMR number such as; 12345.999.000.zip If the compressed file is less than 5M then send via Ecurep e-mail @ sterling_support@ecurep.ibm.com If the compressed file is greater than 5M then send via Ecurep FTP See the section "Submitting Data to IBM Support" for more details. 
     * 
     *  
       
       
       
    
    
    
    

SUBMITTING DATA TO IBM SUPPORT
To diagnose or identify a problem, it is sometimes necessary to provide Technical Support with data and information from your system. In addition, Technical Support might also need to provide you with tools or utilities to be used in problem determination. You can submit files using one of following methods to help speed problem diagnosis:

 * Service Request (SR)
 * E-Mail
 * FTP to the Enhanced Customer Data Repository (ECuRep)
 *  If the compressed file is less than 5M then send via Ecurep e-mail to sterling_support@ecurep.ibm.com 
 *  Enter the following in the subject line PMR 12345.999.000.zip
 *  If the compressed file is greater than 5M then send to Ecurep via FTP

Exchanging information with IBM Technical Support for problem determination [http://www.ibm.com/software/support/exchangeinfo.html]