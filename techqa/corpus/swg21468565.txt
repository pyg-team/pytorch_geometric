Title: IBM TEMS crash and Problematic Situations - United States

Text:
tems crash problem situations TECHNOTE (FAQ)

QUESTION
 How do you recover when a situation is crashing the TEMS? 

ANSWER
Overview
A TEMS crashed shortly after TEMS startup. The TEMS operations log started at 11:48:12 and ended at 11:50:21. In this Linux environment the operations log was in <installdir>/logs and had the form hostname_ms_timestamp.log. At the end of the operations log there were more than 270 sequences like this:

1) KFAST003 Take Action command perl /opt/IBM/ITM/EMail/itmaction.pl <sitname> <agentname> *OFFLINE 1110127114813000 completed with status of 0.
2) Sent email to <email_address>

The actual names were filled in, of course. Line 2 was a standard output print from the Perl script. All standard output lines are collected in the operations log file.

At the very end of the operations log there were messages about the TEMS shutdown starting with this:

KO4SRV017 Tivoli Enterprise Monitoring Server (TEMS) shutdown in progress.

This pattern repeated every time the hub TEMS was started.

In this particular case, the situation formula was determined using a IBM support tool from the database file. You can prepare for such a case by periodically doing a tacmd bulkexportsit operation.

The situation formula was this:

*IF *VALUE ManagedSystem.Status *EQ *OFFLINE *AND 
( ( ( *VALUEManagedSystem.Product *EQ LZ ) *OR 
( *VALUE ManagedSystem.Product *EQ NT) *OR 
( *VALUE ManagedSystem.Product *EQ UX ) ) ) 
*UNTIL ( *TTL 0:02:15:00 )

The formula checked to see if three specific agent types: Linux OS Agent, Windows OS Agent and Unix OS Agent were offline.

There are 4 important observations found during problem resolution and recovery.


Lesson #1 – Never use UNTIL/TTL with sampled events

This had no effect on this case, but it is a terrible idea. ITM sampled situation events will be delivered as a true event and later a false event. If you arbitrarily close the open or true event, the condition will not show as alerted, even if the condition continues. An alert will only be shown after the condition goes false and later true again. The only legitimate way to get a Sampled Situation Event closed is

1) Resolve the condition that caused the alert.
2) Stop the situation.
3) Change the situation distribution to exclude the agent involved.

UNTIL/TTL must only be used with pure events, which never close spontaneously. In many such cases, the event is sent to another event processor like TEC or Omnibus. In other cases the situation results in another form of alert, like an email or a page. In those cases, there is no reason to waste time performing manual operations at the Portal Client to clean up the alerts. However for Sampled events, this case does not arise and can only cause confusion and lost information.


Lesson #2 – Understand the Situation Formula Completely

This situation formula starts off

*IF *VALUE ManagedSystem.Status *EQ *OFFLINE 

This is a good start. However it is not enough. The ManagedSystem attribute group is maintained by the hub TEMS using the node status table INODESTS . Each connected agent periodically [default 10 minutes] updates a table row current timestamp. When the TEMS process notices timestamps that are old, the logic determines that the agent is offline.

At TEMS startup, every agent has an old timestamp. However it makes no sense to generate a flood of offline events followed by a flood of online events in the next ten to twenty minutes. To manage this, the INODESTS table has a Reason field. That field is set to the value FA at startup. Twenty minutes later, the Reason field is set to blanks. 

To make use of this fact, situations must use this form:

*IF *VALUE ManagedSystem.Status *EQ *OFFLINE *AND 
*VALUE ManagedSystem.Reason *NE ‘FA’ *AND ...

That means the status should be offline and also that we must be past the initial TEMS startup period. At that point most operating agents have updated their timestamps and the ones which remain are considered offline.

Now you can see the second flaw in the situation: without the test against Reason, all agents were declared offline immediately.

Note 1: The times mentioned assume the default CTIRA_HEATBEAT value of 600 seconds. It is possible to change this, but most users will not, as every agent would have to be updated. One of the major ITM design guidance is to avoid alerting on transient conditions that may correct themselves. Ten minutes is a reasonable time for most environments. However for special purpose ITM installations, making this smaller can be reasonable. Be sure to do careful testing since IBM does not.

Note 2: There was one site where about 50 agents came online between 20 and 25 minutes past startup. To manage that issue, the customer Offline type situations were made non-autostart and a autostart workflow policy was set to run at the hub TEMS, wait for 30 minutes and then start the customer Offline type situations.

Note 3: FA stands for Framework Architecture, which is the component that does this work.

The lesson here is to make sure you understand the situation formula completely. In this case there was a product provided situation MS_Offline that had the Reason code test. That model should have been followed. The same type of event flooding could result from any not well thought out or tested situation formula.


Lesson #3 – Action Commands Simultaneously.

If the above situation did not have an action command, the result would “just” be an annoying flurry of offline agent alerts and later a flurry of closing alerts as agents came back online. However in this case the TEMS crashed. That introduces a lesson three concerning action commands.

In all the ways command lines can be executed

1) Situation Action commands
2) Workflow Policy Take Action activities
3) Portal Client Take Action process
4) tacmd execuateAction
5) tacmd executeCommand

a command is spawned into a background thread that runs independently of the ITM logic itself using the Posix system() routine. Each such thread consumes a large allocation of process virtual storage space. If this comes close to exceeding the ITM process virtual storage maximum, the ITM process will fail.

In this particular case, there were 274 agents and from the operations log, it appears that about 200 were completed by the time the TEMS crashed. Here are two technotes that can help clarify the issues. This first is a general discussion of action commands:

ITM Action commands Tips and Techniques
http://www-01.ibm.com/support/docview.wss?uid=swg21407744 [http://www-01.ibm.com/support/docview.wss?uid=swg21407744]

The second technote is an example of ensuring that some classes of commands will run one at time. This is not for the faint of heart.

Running Situation Action commands one at a time
http://www-01.ibm.com/support/docview.wss?uid=swg21450660 [http://www-01.ibm.com/support/docview.wss?uid=swg21450660]

In summary, the malformed Offline type situation triggered a large number of action commands rapidly, the TEMS ran out of virtual storage, and the TEMS crashed. This could happen anything there are a large number of action commands at the same time. 

The actual number of commands that cause TEMS failure depends on many factors and cannot be exactly specified. The major contributors are how long an action command takes to complete, how many are created in a short period of time and what is the platform environment process virtual storage limit.


Lesson #4 – Recovering From a Faulty Situation

At this point, we know what the general issue is… but the TEMS still crashed shortly startup and the question is how to recover.

The first step is to configure the hub TEMS to start up in non-SITMON mode:

Running TEMS without SITMON [https://ibm.biz/BdR8nK]

SITMON is the process in TEMS which starts situations, so without it running no situations will start. Using the above technote, reconfigure TEMS and start it.

The following instructions are for a Linux/Unix environment and use the kdstsns program which is delivered with the TEMS program objects. Usually, the KfwSQLClient is preferred for many reasons, but that depends on a working TEPS.

1) Create a driver file for kdstsns named /opt/IBM/ITM/tmp/updsit.txt: 

ip.pipe 
hub-hostname 
1918 
updsit.sql 
end 

Replace the hub_hostname with the ip address or hostname of the TEMS 
hub. If the TEMS only communicates using ip.spipe, it is best to temporarily add ip.pipe to the mix. You can do that with a reconfigure process. 

2) Create a SQL file in /opt/IBM/ITM/tmp directory with the name updsit.sql. The directory /opt/IBM/ITM/tmp is convenient because it is world writeable. The contents are on 4 lines 
UPDATE O4SRV.TSITDESC 
SET AUTOSTART = '*NO' 
WHERE SITNAME = <sitname>'; 
; 

Add an extra semicolon to avoid issues with some editors.

3) Determine the location of the kdstsns program and use it in next step

cd /opt/IBM/ITM ; find . | grep kdstsns

4) Issue these two commands used in a test zLinux environment 

export SQLLIB=/opt/IBM/ITM/tmp

/opt/IBM/ITM/li6263/ms/bin/kdstsns </opt/IBM/ITM/tmp/updsit.txt >/opt/IBM/ITM/tmp/updsit.lst

Review the updsit.lst file for errors. 

When the kdstsns SQL update is verified, stop the TEMS, reverse the no-SITMON changes and restart TEMS.

Note: Windows is almost the same. The kdstsns program is found in <installdir>\bin and you have more flexibility about where files can be stored.



Summary: 

At this point there is a running TEMS and you can change the problematic situation at leisure. Usually that means looking and fixing other examples. If you are lucky, you may never need this procedure.