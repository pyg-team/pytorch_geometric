Title: IBM Interpretation of CHAID results and the predicted target category in AnswerTree and SPSS Trees - United States

Text:
 TECHNOTE (TROUBLESHOOTING)

PROBLEM(ABSTRACT)
 I have built two CHAID decision trees in AnswerTree or with SPSS/Statistics Trees. For one model I didn't partition the file into training and test data, but for the other tree I did. I am wondering why the target category in the trees are different when I look at the parent node in the tree. In the partioned tree the predicted target category is a "No", and in the other one "Yes".
How to interpret these results? 

RESOLVING THE PROBLEM
The target variable is the variable being predicted and whose distribution is displayed in the node statistics. The target category is the value(s) of the target variable which are used to calculate percentages in the gains chart. You can't tell which category is the target category by looking at the tree. 
In AnswerTree, the target category is the highest value of the target variable by default, but you can change that by choosing Gains from the Formats menu in AnswerTree and choosing a different value from the scroll bar under "Gain Column-> Contents->Percentage of cases in target category". 
In SPSS/Statistics TREE, you choose the target category by clicking the Categories button and checking the boxes for the desired categories under 'Target'. In SPSS/Statistics Tree, you can't define a target category unless the target variable has value labels - the Categories button will be greyed out if value labels were not defined. The tree can still be grown and choosing a target variable does not affect tree growth, although it does affect many of the other plots and tables.
In the tree diagram, the category which is the predicted category will be highlighted. The order of the target variable categories in the node (which will be constant across all of the nodes in the tree) does not reflect the predicted value. 
In AnswerTree, the order of categories for a nominal target variable is determined by the order of appearance in the data set. For ordinal target variables, the categories are ordered by value. For example, if the first case in the data had a 0 (No) in the target variable, so the first category in the node statistics in the nonpartitioned tree is 'No'.

When the tree is partitioned, the order of categories in the node statistics is determined by the order of appearance in the training sample. That first case with a value of 'No' must have been assigned to the test sample, and the first case in the training sample for emample had a value of 'Yes'. As a result, 'Yes' is the first value in the node statistics in the partitioned tree. 
As noted above, being first in the list of categories has nothing to do with which category is predicted.

In the absence of user-defined misclassification costs or priors (and CHAID doesn't let you set priors), the predicted category for a node will be the largest category in that node. In your trees, 'No' is always the largest category so it is always the predicted category (in both AnswerTree and TREE). 
When one category is so much larger than the other, it's not unusual for the smaller category to never be the predicted category, even though the percentage of that category may vary widely across terminal nodes. One way to tilt the assignment more in favor of the smaller category is to enter customized misclassification costs. 
The idea is that you can indicate that misclassifying a true 'Yes' as a 'No' has 4 times the cost of misclassifying a true 'No' as a 'Yes'. 
If the ratio of observed 'Yes' to 'No' in a node is greater than 1:4 (i.e. 1 to 4 odds, or a percentage of 'Yes' of 20%) then the node will be predicted to be 'Yes', although 'No' is the larger category, because the cost of assigning it to 'Yes' will be lower than the cost of assigning it to 'No'. 
This makes it possible to detect nodes with 'Yes' percentages that meet some critical threshold that may be less than 50%. The tradeoff for doing this is a large number of 'false positives', true 'No's' that are predicted to be 'Yes's. If it's more important to detect the relatively high 'Yes' nodes, that tradeoff may be acceptable.

To set custom costs in AnswerTree, either click the Advanced Options button while working through the Tree Wizard, or choose Advanced Options from the Analysis menu if the tree (or even the root node) has already been drawn. (If the tree was already drawn, setting costs will cause it be redrawn from the root node.) 
As long as the the target variable is categorical, there will be a Costs tab in the Advanced Options dialog. Click that tab and then the radio button for Custom under 'Misclassification Costs'. The value in the matrix cell where 'Actual category' equals 0 and 'Predicted category' = 1 is the cost of predicting a true 1 to be a 0. If you change that value to 4, then the cost of that misclassification is a 4. If you leave the value in the opposite cell, where the Predicted category is 1 and the actual category is 0, as a 1, you have effectively indicated that the first misclassification has 4 times the cost of the second misclassification. 
It is the ratio of the costs values that is important, rather than the absolute value. Using 8 and 2 would have the same effect as using 4 and 1. The values in the main diagonal cells represent correct classifications so there is no cost and the value is fixed at 0. Click OK when you've finished setting the costs (and any other desired options from the Stopping Rules and CHAID tabs).
To set costs in SPSS TREE, click the Options button and then the Misclassification Costs button. The Misclassification Costs dialog works in the same way as it does in AnswerTree. Unlike AnswerTree, the target variable in SPSS TREE must have value labels for you to enter custom costs. (You won't see a 'Misclassification Costs' tab in the Options dialog if the target variable does not have value labels. )

RELATED INFORMATION
 Need more help? Our Statistics forum is Live! [https://developer.ibm.com/answers/topics/statistics.html?smartspace=predictive-analytics]


 

HISTORICAL NUMBER
 80173