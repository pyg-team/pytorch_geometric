import torch
import torch.nn as nn

from torch_geometric.nn import MessagePassing


class GeoGNNConv(torch.nn.Module):
    """Represents a single layer for a single graph of the GeoGNN model
    from the paper `Geometry-enhanced molecular representation learning
    for property prediction`
    <https://www.nature.com/articles/s42256-021-00438-4>`_.

    Args:
        embed_dim (int): The dimensionality of the input and output
            embeddings.
        dropout_rate (float): The dropout rate to apply to the output
            embeddings.
        last_act (bool): Whether to apply an activation function to the
            output embeddings.

    Returns:
        torch.Tensor: The output embeddings after applying the GeoGNNConv.
    """
    def __init__(self, embed_dim, dropout_rate, last_act=False):
        super(GeoGNNConv, self).__init__()

        self.embed_dim = embed_dim
        self.last_act = last_act

        self.geo_conv = ModGINConv(embed_dim, embed_dim)

        self.graph_norm = GraphNorm()
        self.act = nn.ReLU() if last_act else None
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(self, x, edge_index, batch, edge_attr=None):
        """Performs a forward pass of the GeoGNNConv.

        Args:
            x (torch.Tensor): The input embeddings.
            edge_index (torch.Tensor): The edge indices of the graph.
            batch (torch.Tensor): The batch indices of the graph.
            edge_attr (torch.Tensor or None): The edge attributes of
                the graph.

        Returns:
            torch.Tensor: The output embeddings after applying
                the GeoGNNConv.
        """
        out = self.geo_conv(x, edge_index, edge_attr)
        out = self.graph_norm(out, batch)
        if self.last_act:
            out = self.act(out)
        out = self.dropout(out)
        out = out + x  # Residual connection
        return out


class ModGINConv(MessagePassing):
    """Modified GIN used for GeoConv.
    Node and edge features are generated by linear transform.

    Args:
        in_channels (int): Number of input channels (node features).
        out_channels (int): Number of output channels (node features).

    Returns:
        torch.Tensor: Updated node features after message passing,
            aggregation, and node feature update.
    """
    def __init__(self, in_channels, out_channels):
        super(ModGINConv, self).__init__(aggr='add')  # "Add" aggregation.
        self.lin = torch.nn.Linear(in_channels, out_channels)
        self.edge_lin = torch.nn.Linear(in_channels, out_channels)
        self.layer_norm = nn.LayerNorm(out_channels)

    def forward(self, x, edge_index, edge_attr):
        """Forward pass of the ModGINConv layer.

        Args:
            x (torch.Tensor): Input node features.
            edge_index (torch.Tensor): Edge indices.
            edge_attr (torch.Tensor): Edge attributes.

        Returns:
            torch.Tensor: Updated node features after message passing,
                aggregation, and node feature update.
        """
        x = self.lin(x)
        edge_attr = self.edge_lin(edge_attr)
        return self.propagate(edge_index, x=x, edge_attr=edge_attr)

    def message(self, x_j, edge_attr):
        """Message function for the ModGINConv layer.

        Args:
            x_j (torch.Tensor): Node features of neighboring nodes.
            edge_attr (torch.Tensor): Edge attributes.

        Returns:
            torch.Tensor: Messages passed from neighboring nodes.
        """
        return x_j + edge_attr

    def update(self, aggr_out, x):
        """Update function for the ModGINConv layer.

        Args:
            aggr_out (torch.Tensor): Aggregated node features.
            x (torch.Tensor): Original node features.

        Returns:
            torch.Tensor: Updated node features after applying layer
                normalization and residual connection.
        """
        aggr_out = self.layer_norm(aggr_out)
        return aggr_out + x


class GraphNorm(nn.Module):
    """Normalizes node features in a graph by dividing by the squareroot
    of the number of nodes in the graph. Supports batched data.

    Methods:
        forward(x, batch): Performs the forward pass of the GraphNorm module.

    Example:
        graph_norm = GraphNorm()
        output = graph_norm(input, batch)
    """
    def __init__(self):
        super(GraphNorm, self).__init__()

    def forward(self, x, batch):
        num_nodes = torch.bincount(batch)
        return x / num_nodes[batch].sqrt().unsqueeze(-1)
